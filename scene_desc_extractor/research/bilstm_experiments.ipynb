{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, BatchSampler\n",
    "import torch.optim as optim\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from nltk.tokenize import sent_tokenize"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T10:10:16.522677147Z",
     "start_time": "2024-04-10T10:10:16.479542107Z"
    }
   },
   "id": "a56b1e78bc22443d",
   "execution_count": 893
  },
  {
   "cell_type": "markdown",
   "source": [
    "The get_lstm_features function returns the LSTMâ€™s tag vectors. The function performs all the steps mentioned above for the model.\n",
    "Steps:\n",
    "1) It takes in characters, converts them to embeddings using our character LSTM.\n",
    "2) We concat Character Embeding with embeding word vectors, use this as features that we feed to Bidirectional-LSTM.\n",
    "3) The Bidirectional-LSTM generates outputs based on these set of features.\n",
    "4) The output are passed through a linear layer to convert to tag space."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "35cd2a4d1d551682"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "START_TAG = 'B'\n",
    "END_TAG = 'E'\n",
    "NO_ENTITY_TAG = 'O'\n",
    "\n",
    "START_SENTENCE_TOKEN = '[CLS]'\n",
    "SEP_SENTENCE_TOKEN = '[SEP]'\n",
    "PAD_TOKEN = '[PAD]'\n",
    "UNKNOWN_TOKEN = '[UNK]'\n",
    "\n",
    "START_SENTENCE_TOKEN_ID = 101\n",
    "SEP_SENTENCE_TOKEN_ID = 102\n",
    "PAD_TOKEN_ID = 0\n",
    "UNKNOWN_TOKEN_ID = 100\n",
    "\n",
    "SPECIAL_TOKENS = [START_SENTENCE_TOKEN, SEP_SENTENCE_TOKEN, PAD_TOKEN, UNKNOWN_TOKEN]\n",
    "START_TEXT_TOKEN = '[PST]'\n",
    "END_TEXT_TOKEN = '[PEN]'"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T07:54:08.166561902Z",
     "start_time": "2024-04-11T07:54:08.125342326Z"
    }
   },
   "id": "acb2f1e361b92d5e",
   "execution_count": 1038
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 11, 11, 11, 12, 13, 14, 14, 14, 15, 15, None]\n"
     ]
    },
    {
     "data": {
      "text/plain": "16"
     },
     "execution_count": 1009,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = 'This is an example text. It demonstrates how to use embeddings layers or embedding lookup'\n",
    "labels = ['O', 'O', 'B-A', 'I-A', 'E-A', 'O', 'B-A', 'O', 'O', 'O', 'O', 'B-A', 'E-A', 'O', 'B-A', 'E-A']\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', return_tensors='pt')\n",
    "t = tokenizer(s, return_tensors='pt')\n",
    "input_ids = t['input_ids'].squeeze().tolist()\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "word_ids = []\n",
    "\n",
    "# Get the word IDs\n",
    "word_ids = []\n",
    "current_word_id = -1\n",
    "for i, token_id in enumerate(input_ids):\n",
    "    if token_id == tokenizer.cls_token_id or token_id == tokenizer.sep_token_id:\n",
    "        word_ids.append(None)  # Special tokens like [CLS] and [SEP] have no corresponding word ID\n",
    "        continue\n",
    "    \n",
    "    if tokens[i].startswith(\"##\"):        \n",
    "        word_ids.append(current_word_id)\n",
    "        continue\n",
    "        \n",
    "    current_word_id += 1\n",
    "    word_ids.append(current_word_id)\n",
    "        \n",
    "print(word_ids)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T11:39:14.273428959Z",
     "start_time": "2024-04-10T11:39:14.136295065Z"
    }
   },
   "id": "e3a444fbc417855f",
   "execution_count": 1009
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Aligned Tokens: ['this', 'is', 'an', 'example', 'text', '.', 'it', 'demonstrates', 'how', 'to', 'use', 'em', 'layers', 'or', 'em', 'look']\n",
      "Aligned Labels: ['O', 'O', 'B-A', 'I-A', 'E-A', 'O', 'B-A', 'O', 'O', 'O', 'O', 'B-A', 'B-A', 'E-A', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "# Initialize the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input sentence and corresponding labels\n",
    "# input_sentence = \"John lives in New York City.\"\n",
    "# labels = [\"B-PER\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"I-LOC\", \"O\"]\n",
    "input_sentence = 'This is an example text. It demonstrates how to use embeddings layers or embedding lookup'\n",
    "labels = ['O', 'O', 'B-A', 'I-A', 'E-A', 'O', 'B-A', 'O', 'O', 'O', 'O', 'B-A', 'E-A', 'O', 'B-A', 'E-A']\n",
    "\n",
    "# Tokenize the input sentence\n",
    "tokenized_input = tokenizer.tokenize(input_sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "\n",
    "# Initialize lists to store aligned tokens and labels\n",
    "aligned_tokens = []\n",
    "aligned_labels = []\n",
    "\n",
    "# Initialize variables to keep track of token and label positions\n",
    "token_pos = 0\n",
    "label_pos = 0\n",
    "\n",
    "# Loop through the tokenized input and align tokens with labels\n",
    "while token_pos < len(token_ids):\n",
    "    token = tokenized_input[token_pos]\n",
    "    token_label = labels[label_pos]\n",
    "\n",
    "    # Skip special tokens like [CLS], [SEP], [PAD]\n",
    "    if token.startswith(\"##\"):\n",
    "        token_pos += 1\n",
    "        continue\n",
    "\n",
    "    # Add aligned token and label to the lists\n",
    "    aligned_tokens.append(token)\n",
    "    aligned_labels.append(token_label)\n",
    "\n",
    "    # Move to the next token and label positions\n",
    "    token_pos += 1\n",
    "\n",
    "    # If the current token is part of a word (e.g., \"New\" and \"York\" in \"New York City\"),\n",
    "    # increment the label position to match the next word\n",
    "    if not token.endswith(\"##\") and token_pos < len(token_ids) and not tokenized_input[token_pos].startswith(\"##\"):\n",
    "        label_pos += 1\n",
    "\n",
    "print(len(token_ids))\n",
    "print(\"Aligned Tokens:\", aligned_tokens)\n",
    "print(\"Aligned Labels:\", aligned_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-10T11:55:52.044655019Z",
     "start_time": "2024-04-10T11:55:51.911215486Z"
    }
   },
   "id": "d5a4d8897928d209",
   "execution_count": 1014
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Initialize the BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Example input sentence and corresponding labels\n",
    "input_sentence = \"John lives in New York City.\"\n",
    "labels = [\"B-PER\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"I-LOC\", \"O\"]\n",
    "\n",
    "# Tokenize the input sentence\n",
    "tokenized_input = tokenizer.tokenize(input_sentence)\n",
    "token_ids = tokenizer.convert_tokens_to_ids(tokenized_input)\n",
    "\n",
    "# Convert the token IDs to tensor\n",
    "input_ids = torch.tensor(token_ids).unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Get token embeddings from the BERT model\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids)\n",
    "    token_embeddings = outputs.last_hidden_state\n",
    "\n",
    "# Align token embeddings with labels\n",
    "aligned_embeddings = []\n",
    "aligned_labels = []\n",
    "\n",
    "# Iterate through the token embeddings and align with labels\n",
    "for idx, (token, label) in enumerate(zip(tokenized_input, labels)):\n",
    "    # Skip special tokens like [CLS], [SEP], [PAD]\n",
    "    if token.startswith(\"##\"):\n",
    "        continue\n",
    "    \n",
    "    # Add the token embedding and label to the aligned lists\n",
    "    aligned_embeddings.append(token_embeddings[0, idx + 1])  # Skip [CLS] token at idx 0\n",
    "    aligned_labels.append(label)\n",
    "\n",
    "print(\"Number of aligned embeddings:\", len(aligned_embeddings))\n",
    "print(\"Number of aligned labels:\", len(aligned_labels))"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb90b9c7bd560a86"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class MultiSentenceDataset(Dataset):\n",
    "    def __init__(self, file_dir, labels_dir=None, uncased=True, \n",
    "                 bert_model_name='bert-base-uncased', max_token_length=15):\n",
    "        self.file_dir = file_dir\n",
    "        self.labels_dir = labels_dir\n",
    "        self.uncased = uncased\n",
    "        self.max_token_length = max_token_length\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(bert_model_name)\n",
    "        self.texts_sentences, self.texts_mask, self.sentence_mask, self.labels = self.__read_data()\n",
    "        self.tokens_ids, self.sentence_lengths, self.word_tokens, self.word_ids = self.__tokenize_sentences()\n",
    "        self.chars_dict = self.__get_chars_dict()        \n",
    "        self.chars_vocab_dim = len(self.chars_dict)\n",
    "        self.chars_seq, self.chars_lengths = self.__tokenize_and_pad_characters_per_token()\n",
    "        \n",
    "        \n",
    "    def __read_data(self):\n",
    "        texts_sentences = []\n",
    "        text_mask = []\n",
    "        sentence_mask = []\n",
    "        labels = []        \n",
    "        text_enumerator = 0        \n",
    "        for filename in os.listdir(self.file_dir):\n",
    "            with open(os.path.join(self.file_dir, filename), 'r', encoding='utf-8') as file:                \n",
    "                text = file.read()\n",
    "                sentences = sent_tokenize(text)\n",
    "                sentence_mask.extend(range(len(sentences)))\n",
    "                text_mask.extend([text_enumerator]*len(sentences))\n",
    "                texts_sentences.extend(sentences)\n",
    "                \n",
    "                text_enumerator += 1\n",
    "            # if self.labels_dir:\n",
    "            #     with open(os.path.join(self.labels_dir, filename), 'r', encoding='utf-8') as file:\n",
    "            #         label = file.read()                    \n",
    "            #         labels.append(NO_ENTITY_TAG + ' ' + label + ' ' + NO_ENTITY_TAG)\n",
    "        return texts_sentences, text_mask, sentence_mask, labels if self.labels_dir else None\n",
    "    \n",
    "    def __get_words_from_tokens(self, sentence_tokens_ids):\n",
    "        word_tokens = []\n",
    "        word_ids = []\n",
    "        tokens = self.tokenizer.convert_ids_to_tokens(sentence_tokens_ids)  \n",
    "        word_id = -1        \n",
    "        for token in tokens:\n",
    "            if token.startswith(\"##\"):\n",
    "                word_tokens[-1] += token[2:]\n",
    "            else:         \n",
    "                word_id += 1\n",
    "                word_tokens.append(token)\n",
    "            word_ids.append(word_id)            \n",
    "        return word_tokens, word_ids\n",
    "        \n",
    "    def __tokenize_sentences(self):\n",
    "        tokens_ids = []\n",
    "        word_tokens = []\n",
    "        word_ids = []\n",
    "        sentence_lengths = []\n",
    "        for i, sentence in enumerate(self.texts_sentences):\n",
    "            tokenized_sentence = self.tokenizer(sentence, return_tensors='pt') \n",
    "            sentence_tokens_ids = tokenized_sentence['input_ids'].squeeze().tolist()            \n",
    "            tokens_ids.append(sentence_tokens_ids)\n",
    "            \n",
    "            sentence_word_tokens, sentence_word_ids = self.__get_words_from_tokens(sentence_tokens_ids)\n",
    "            word_tokens.append(sentence_word_tokens)\n",
    "            word_ids.append(sentence_word_ids)\n",
    "            sentence_lengths.append(len(sentence_word_tokens))\n",
    "            \n",
    "        return tokens_ids, sentence_lengths, word_tokens, word_ids\n",
    "\n",
    "    \n",
    "    def __tokenize_and_pad_characters_per_token(self):\n",
    "        encoded_char_seq = []\n",
    "        chars_lengths = []\n",
    "            \n",
    "        for sentence_index, sentence_length in enumerate(self.sentence_lengths):    \n",
    "            token_char_ids = []\n",
    "            token_lengths = [] \n",
    "            for token_index in range(sentence_length):\n",
    "                token = self.word_tokens[sentence_index][token_index]       \n",
    "                if token in SPECIAL_TOKENS:\n",
    "                    token_char_ids.append(([self.chars_dict[token]]))\n",
    "                else:\n",
    "                    token_char_ids.append(([self.chars_dict[c] for c in token]))         \n",
    "                length = len(token_char_ids[-1])\n",
    "                token_lengths.append(length)   \n",
    "            encoded_char_seq.append(token_char_ids)\n",
    "            chars_lengths.append(token_lengths)\n",
    "        \n",
    "        padded_encoded_char_seq = [[t + [0] * (self.max_token_length - len(t)) for t in s] for s in encoded_char_seq]\n",
    "        return padded_encoded_char_seq, chars_lengths\n",
    "    \n",
    "    \n",
    "    def __get_chars_dict(self):\n",
    "        chars_freq = {START_SENTENCE_TOKEN: len(self.word_tokens), SEP_SENTENCE_TOKEN: len(self.word_tokens), \n",
    "             PAD_TOKEN: 0, UNKNOWN_TOKEN: 0}\n",
    "        \n",
    "        for sentence in self.word_tokens:\n",
    "            for i, token in enumerate(sentence):\n",
    "                if (token == START_SENTENCE_TOKEN) or (token == SEP_SENTENCE_TOKEN):\n",
    "                    continue\n",
    "                if token == PAD_TOKEN:\n",
    "                    break                               \n",
    "                if token == UNKNOWN_TOKEN:\n",
    "                    chars_freq[token] += 1\n",
    "                else: \n",
    "                    for c in token:\n",
    "                        if c in chars_freq:\n",
    "                            chars_freq[c] += 1\n",
    "                        else:\n",
    "                            chars_freq[c] = 0\n",
    "        chars = dict(sorted(chars_freq.items(), key=lambda item: item[1], reverse=True))\n",
    "        del chars[PAD_TOKEN]\n",
    "        chars_vocab = {PAD_TOKEN: 0}\n",
    "        chars_vocab.update(dict((item, i+1) for i, item in enumerate(chars)))\n",
    "        return chars_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.tokens_ids)\n",
    "\n",
    "    def __getitem__(self, idx):   \n",
    "        if self.labels is not None:\n",
    "            pass\n",
    "        else:\n",
    "            return {\n",
    "                    'token_ids': self.tokens_ids[idx], \n",
    "                    'words': self.word_tokens[idx],\n",
    "                    'word_ids': self.word_ids[idx],\n",
    "                    'sentence_length':  self.sentence_lengths[idx],\n",
    "                    'text_mask': self.texts_mask[idx],\n",
    "                    'sentence_mask': self.sentence_mask[idx],\n",
    "                    'word_char_ids': self.chars_seq[idx],\n",
    "                    'word_lengths': self.chars_lengths[idx],\n",
    "                    }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T08:58:24.022711600Z",
     "start_time": "2024-04-11T08:58:23.980294693Z"
    }
   },
   "id": "e021045eb97f6ffb",
   "execution_count": 1059
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def sentence_data_collate_fn(batch):\n",
    "    text_mask = []\n",
    "    sentence_mask = []\n",
    "    sentence_length = []\n",
    "    \n",
    "    token_ids = []\n",
    "    token_mask = []    \n",
    "    word_ids = []\n",
    "    \n",
    "    word_char_ids = []\n",
    "    word_lengths = []\n",
    "    \n",
    "    max_sentence_length = 0\n",
    "    max_tokens_count = 0\n",
    "    \n",
    "    for sentence in batch:\n",
    "        if sentence['sentence_length'] > max_sentence_length:\n",
    "            max_sentence_length = sentence['sentence_length']\n",
    "        if len(sentence['token_ids']) > max_tokens_count:\n",
    "            max_tokens_count = len(sentence['token_ids'])      \n",
    "        sentence_length.append(sentence['sentence_length'])\n",
    "        text_mask.append(sentence['text_mask'])\n",
    "        sentence_mask.append(sentence['sentence_mask'])        \n",
    "    \n",
    "    for i, sentence in enumerate(batch):\n",
    "        missing_sentence_length = max_sentence_length - sentence['sentence_length']\n",
    "        missing_tokens_count = max_tokens_count - len(sentence['token_ids'])\n",
    "        token_ids.append(sentence['token_ids'] + [0]*missing_tokens_count)\n",
    "        word_ids.append(sentence['word_ids'] + [-1]*missing_tokens_count)\n",
    "        token_mask.append([1]*len(sentence['token_ids']) + [0]*missing_tokens_count)\n",
    "\n",
    "        padded_chars = sentence['word_char_ids'] + [[0]*MAX_TOKEN_LENGTH]*missing_sentence_length\n",
    "        word_char_ids.append(padded_chars)\n",
    "        word_lengths.append(sentence['word_lengths'] + [1]*missing_sentence_length)\n",
    "\n",
    "    return {\n",
    "                'token_ids': torch.tensor(token_ids), \n",
    "                'token_mask':  torch.tensor(token_mask),\n",
    "                'word_ids': torch.tensor(word_ids),         \n",
    "                'text_mask': torch.tensor(text_mask),\n",
    "                'sentence_mask': torch.tensor(sentence_mask),\n",
    "                'sentence_length': torch.tensor(sentence_length),\n",
    "                'word_char_ids': torch.tensor(word_char_ids),\n",
    "                'word_lengths': torch.tensor(word_lengths),\n",
    "            }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T09:04:23.328948715Z",
     "start_time": "2024-04-11T09:04:23.318595685Z"
    }
   },
   "id": "6e732829d51346a5",
   "execution_count": 1067
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class WordBertEmbeddingModel(nn.Module):\n",
    "    def __init__(self, bert_model_name='bert-base-uncased', pretrain=False, average_hidden_states=False):        \n",
    "        super(WordBertEmbeddingModel, self).__init__()\n",
    "        self.average_hidden_states = average_hidden_states\n",
    "        self.model = BertModel.from_pretrained(bert_model_name)   \n",
    "        self.embeddings_dim = self.model.config.hidden_size\n",
    "        \n",
    "        if not pretrain:\n",
    "            for param in self.model.parameters():\n",
    "                param.requires_grad = False  \n",
    "    \n",
    "#     word_ids = batch['word_ids']\n",
    "# token_embeddings = word_embeddings\n",
    "# max_words_count = torch.max(batch['sentence_length']).item()\n",
    "# embeddings_dim = 768\n",
    "# \n",
    "# embeddings = []\n",
    "# for sentence, sentence_embed in zip(word_ids, token_embeddings):\n",
    "#     prev_id = -1\n",
    "#     sentence_embeddings = []\n",
    "#     prev_embedding = None\n",
    "#     collected_embeddings = []\n",
    "#     for word_id, token_embed in zip(sentence, sentence_embed):\n",
    "#         if word_id == -1:\n",
    "#             break\n",
    "#             \n",
    "#         if prev_id == -1:\n",
    "#             print(f'prev_id: {prev_id}, prev_emb: {None}')\n",
    "#             prev_id = word_id\n",
    "#             prev_embedding = token_embed            \n",
    "#             continue\n",
    "#             \n",
    "#         if word_id == prev_id:\n",
    "#             collected_embeddings.append(prev_embedding)\n",
    "#             print(f'prev_id: {prev_id}, prev_emb: {prev_embedding[:5]}, word_id: {word_id}, coll: {len(collected_embeddings)}')\n",
    "#         else:\n",
    "#             if len(collected_embeddings) > 0:\n",
    "#                 collected_embeddings.append(prev_embedding)\n",
    "#                 mean_emb = torch.tensor(np.mean(collected_embeddings, axis=0))\n",
    "#                 sentence_embeddings.append(mean_emb)\n",
    "#                 print(f'prev_id: {prev_id}, prev_emb: {prev_embedding[:5]}, word_id: {word_id}, coll: {len(collected_embeddings)}, mean_emb: {mean_emb[0:5]} {len(mean_emb)}')\n",
    "#                 collected_embeddings = []\n",
    "#             else:\n",
    "#                 print(f'prev_id: {prev_id}, prev_emb: {prev_embedding[:5]}, word_id: {word_id}, coll: {len(collected_embeddings)}')\n",
    "#                 sentence_embeddings.append(prev_embedding)\n",
    "#             prev_id = word_id\n",
    "#         prev_embedding = token_embed\n",
    "# \n",
    "# \n",
    "#     if len(collected_embeddings) > 0:\n",
    "#         collected_embeddings.append(prev_embedding)\n",
    "#         sentence_embeddings.append(torch.tensor(np.mean(collected_embeddings, axis=0)))\n",
    "#     else:\n",
    "#         sentence_embeddings.append(prev_embedding)          \n",
    "#     \n",
    "#     embeddings.append(torch.stack(sentence_embeddings))\n",
    "# \n",
    "# \n",
    "# for i, length in enumerate(batch['sentence_length']):\n",
    "#     if length.item() < max_words_count:\n",
    "#         embeddings[i] = torch.cat((embeddings[i], torch.zeros(max_words_count - length.item(), embeddings_dim)), dim=0)\n",
    "# \n",
    "# res = torch.stack(embeddings)\n",
    "\n",
    "                \n",
    "    def __get_mean_embeddings_for_words(self, word_ids, token_embeddings):\n",
    "        max_words_count = torch.max(batch['sentence_length']).item()\n",
    "        embeddings = []\n",
    "        for sentence, sentence_embed in zip(word_ids, token_embeddings):\n",
    "            prev_id = -1\n",
    "            sentence_embeddings = []\n",
    "            prev_embedding = None\n",
    "            collected_embeddings = []\n",
    "            for word_id, token_embed in zip(sentence, sentence_embed):\n",
    "                if word_id == -1:\n",
    "                    break\n",
    "                    \n",
    "                if prev_id == -1:\n",
    "                    prev_id = word_id\n",
    "                    prev_embedding = token_embed\n",
    "                    continue\n",
    "                    \n",
    "                if word_id == prev_id:\n",
    "                    collected_embeddings.append(prev_embedding)                    \n",
    "                else:\n",
    "                    if len(collected_embeddings) > 0:\n",
    "                        collected_embeddings.append(prev_embedding)\n",
    "                        sentence_embeddings.append(torch.tensor(np.mean(collected_embeddings, axis=0)))\n",
    "                        collected_embeddings = []\n",
    "                    else:\n",
    "                        sentence_embeddings.append(prev_embedding)\n",
    "                    prev_id = word_id                    \n",
    "                prev_embedding = token_embed\n",
    "            \n",
    "            if len(collected_embeddings) > 0:\n",
    "                collected_embeddings.append(prev_embedding)\n",
    "                sentence_embeddings.append(torch.tensor(np.mean(collected_embeddings, axis=0)))\n",
    "            else:\n",
    "                sentence_embeddings.append(prev_embedding)          \n",
    "               \n",
    "            embeddings.append(torch.stack(sentence_embeddings))\n",
    "          \n",
    "        \n",
    "        for i, length in enumerate(batch['sentence_length']):\n",
    "            if length.item() < max_words_count:\n",
    "                embeddings[i] = torch.cat((embeddings[i], torch.zeros(max_words_count - length.item(), embeddings_dim)), dim=0)  \n",
    "            \n",
    "        return torch.stack(embeddings)\n",
    "                    \n",
    "        \n",
    "    def forward(self, batch):\n",
    "        x = self.model(input_ids= batch['token_ids'], attention_mask=batch['token_mask'])  \n",
    "        if not self.average_hidden_states:\n",
    "            x = x['last_hidden_state']\n",
    "        else:\n",
    "            x = torch.mean(torch.stack(x.hidden_states), dim=0)\n",
    "        x = self.__get_mean_embeddings_for_words(batch['word_ids'], x)\n",
    "        return x        "
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T10:20:32.226600156Z",
     "start_time": "2024-04-11T10:20:32.184042087Z"
    }
   },
   "id": "d26ca4b054bdadfa",
   "execution_count": 1230
  },
  {
   "cell_type": "markdown",
   "source": [
    "Char representation per word"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "10ed301d70bd86e5"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CharLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_dim, embedding_dim, output_dim, bidirectional=True, num_layers=1):        \n",
    "        super(CharLSTMModel, self).__init__()              \n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.embedding_layer = nn.Embedding(vocab_dim, embedding_dim, padding_idx=0)\n",
    "        self.lstm_layer = nn.LSTM(embedding_dim, output_dim // 2 if bidirectional else output_dim, \n",
    "                                  bidirectional=bidirectional, \n",
    "                                  num_layers=num_layers, batch_first=True)        \n",
    "    def forward(self, batch):\n",
    "\n",
    "        (batch_size, sentence_max_length, token_max_length) = batch['word_char_ids'].shape\n",
    "        x = self.embedding_layer(batch['word_char_ids'])\n",
    "        flat_token_length = batch['word_lengths'].view(-1)\n",
    "        x = x.view(batch_size * sentence_max_length, token_max_length, self.embedding_dim)\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, flat_token_length, batch_first=True, enforce_sorted=False)\n",
    "        x, _ = self.lstm_layer(x)\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, total_length=token_max_length)\n",
    "        #x = x.view(batch_size, sentence_max_length, token_max_length, x.shape[-1])\n",
    "        x = x.view(batch_size, sentence_max_length, -1)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T10:23:17.911603072Z",
     "start_time": "2024-04-11T10:23:17.843531569Z"
    }
   },
   "id": "b9e14285e7fe007a",
   "execution_count": 1239
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class ContextLSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, bidirectional=True, num_layers=1,\n",
    "                 input_dropout_rate=0.5, hidden_dropout_rate=0.5, output_dropout_rate=0.5,\n",
    "                 init_hidden_to_random = True):\n",
    "        super(ContextLSTMModel, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.bidirectional = bidirectional\n",
    "        self.num_layers = num_layers\n",
    "        self.init_hidden_to_random = init_hidden_to_random\n",
    "\n",
    "        #LSTM layers\n",
    "        self.lstm_layer = nn.LSTM(input_dim, hidden_dim, num_layers=num_layers, \n",
    "                            bidirectional=bidirectional, batch_first=True, \n",
    "                            dropout=hidden_dropout_rate if num_layers > 1 else 0)\n",
    "        \n",
    "        #Dropout layers for input and output\n",
    "        if input_dropout_rate and input_dropout_rate > 0:\n",
    "            self.dropout_input_layer = nn.Dropout(input_dropout_rate)\n",
    "        \n",
    "        if output_dropout_rate and output_dropout_rate > 0:\n",
    "            self.dropout_output_layer = nn.Dropout(output_dropout_rate)\n",
    "            \n",
    "        #FC layer to map the LSTM output of into output space \n",
    "        self.fc_layer = nn.Linear(hidden_dim * (2 if bidirectional else 1), output_dim)\n",
    "\n",
    "    def init_hidden_state(self, batch_size):\n",
    "        if self.init_hidden_to_random:\n",
    "            return torch.randn((2 if self.bidirectional else 1) * self.num_layers, batch_size, self.hidden_dim)\n",
    "        else:\n",
    "            return torch.zeros((2 if self.bidirectional else 1) * self.num_layers, batch_size, self.hidden_dim)\n",
    "    \n",
    "    def forward(self, sentences, sentence_lengths):\n",
    "        #per batch size\n",
    "        hidden = self.init_hidden_state(sentences.shape[0])\n",
    "        state = self.init_hidden_state(sentences.shape[0])\n",
    "        word_max_length = sentences.shape[-1]\n",
    "        \n",
    "        x = sentences\n",
    "        if self.dropout_input_layer:\n",
    "            x = self.dropout_input_layer(x)\n",
    "\n",
    "        x = torch.nn.utils.rnn.pack_padded_sequence(x, sentence_lengths, batch_first=True, enforce_sorted=False)\n",
    "        x, (self.hidden, self.state) = self.lstm_layer(x, (hidden, state))\n",
    "        x, _ = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True, total_length=word_max_length)\n",
    "        \n",
    "        if self.dropout_output_layer:\n",
    "            x = self.dropout_output_layer(x)            \n",
    "        x = self.fc_layer(x)        \n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T08:52:50.763243744Z",
     "start_time": "2024-04-11T08:52:50.720031861Z"
    }
   },
   "id": "e7f7f30d8893241f",
   "execution_count": 1052
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "bert_model_name = 'bert-base-uncased'\n",
    "file_dir = 'data_exp/texts'  # Directory containing text files\n",
    "#labels_dir = 'data_exp/labels'  \n",
    "BATCH_SIZE = 2\n",
    "CHAR_EMBEDDING_DIM = 10\n",
    "CHAR_LSTM_OUTPUT = 20\n",
    "CONTEXT_HIDDEN_DIM = 20\n",
    "NUM_LABELS = 10\n",
    "MAX_TOKEN_LENGTH = 15\n",
    "\n",
    "train_dataset = MultiSentenceDataset(file_dir, \n",
    "                                     bert_model_name=bert_model_name, \n",
    "                                     uncased=True)\n",
    "\n",
    "char_vocab_dim = train_dataset.chars_vocab_dim\n",
    "\n",
    "word_model = WordBertEmbeddingModel(bert_model_name=bert_model_name)\n",
    "word_embedding_dim = word_model.embeddings_dim\n",
    "\n",
    "char_model = CharLSTMModel(vocab_dim=char_vocab_dim,\n",
    "                           embedding_dim=CHAR_EMBEDDING_DIM,\n",
    "                           output_dim=CHAR_LSTM_OUTPUT,                           \n",
    "                           )\n",
    "\n",
    "context_model = ContextLSTMModel(input_dim=word_model.embeddings_dim + CHAR_LSTM_OUTPUT*MAX_TOKEN_LENGTH, \n",
    "                                 hidden_dim=CONTEXT_HIDDEN_DIM,\n",
    "                                 output_dim=NUM_LABELS)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T10:23:22.644373154Z",
     "start_time": "2024-04-11T10:23:21.898212078Z"
    }
   },
   "id": "ccaa3012835d9fe9",
   "execution_count": 1240
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index:  0\n",
      "Token IDs Shape: torch.Size([2, 9])\n",
      "Token Attention Mask Shape: torch.Size([2, 9])\n",
      "Word IDs Shape: torch.Size([2, 9])\n",
      "Text mask Shape: torch.Size([2])\n",
      "Sentence mask Shape: torch.Size([2])\n",
      "Sentence length Shape: torch.Size([2])\n",
      "Word chars Shape: torch.Size([2, 9, 15])\n",
      "Word lengths Shape: torch.Size([2, 9])\n",
      "Embedded IDs Shape: torch.Size([2, 9, 768])\n",
      "char_embeddings Shape: torch.Size([2, 9, 300])\n",
      "word_char_embeddings Shape: torch.Size([2, 9, 1068])\n",
      "context_embeddings Shape: torch.Size([2, 1068, 10])\n",
      "Index:  1\n",
      "Token IDs Shape: torch.Size([2, 10])\n",
      "Token Attention Mask Shape: torch.Size([2, 10])\n",
      "Word IDs Shape: torch.Size([2, 10])\n",
      "Text mask Shape: torch.Size([2])\n",
      "Sentence mask Shape: torch.Size([2])\n",
      "Sentence length Shape: torch.Size([2])\n",
      "Word chars Shape: torch.Size([2, 10, 15])\n",
      "Word lengths Shape: torch.Size([2, 10])\n",
      "Embedded IDs Shape: torch.Size([2, 10, 768])\n",
      "char_embeddings Shape: torch.Size([2, 10, 300])\n",
      "word_char_embeddings Shape: torch.Size([2, 10, 1068])\n",
      "context_embeddings Shape: torch.Size([2, 1068, 10])\n",
      "Index:  2\n",
      "Token IDs Shape: torch.Size([2, 14])\n",
      "Token Attention Mask Shape: torch.Size([2, 14])\n",
      "Word IDs Shape: torch.Size([2, 14])\n",
      "Text mask Shape: torch.Size([2])\n",
      "Sentence mask Shape: torch.Size([2])\n",
      "Sentence length Shape: torch.Size([2])\n",
      "Word chars Shape: torch.Size([2, 10, 15])\n",
      "Word lengths Shape: torch.Size([2, 10])\n",
      "Embedded IDs Shape: torch.Size([2, 10, 768])\n",
      "char_embeddings Shape: torch.Size([2, 10, 300])\n",
      "word_char_embeddings Shape: torch.Size([2, 10, 1068])\n",
      "context_embeddings Shape: torch.Size([2, 1068, 10])\n"
     ]
    }
   ],
   "source": [
    "seq_sampler = torch.utils.data.sampler.SequentialSampler(train_dataset)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, \n",
    "                              sampler=seq_sampler, collate_fn=sentence_data_collate_fn)\n",
    "\n",
    "for index, batch in enumerate(train_dataloader):\n",
    "    print('Index: ', index)\n",
    "    print(\"Token IDs Shape:\", batch['token_ids'].shape)\n",
    "    print(\"Token Attention Mask Shape:\", batch['token_mask'].shape)\n",
    "    print(\"Word IDs Shape:\", batch['word_ids'].shape)\n",
    "    print(\"Text mask Shape:\", batch['text_mask'].shape)\n",
    "    print(\"Sentence mask Shape:\", batch['sentence_mask'].shape)    \n",
    "    print(\"Sentence length Shape:\", batch['sentence_length'].shape)    \n",
    "    print(\"Word chars Shape:\", batch['word_char_ids'].shape)\n",
    "    print(\"Word lengths Shape:\", batch['word_lengths'].shape)\n",
    "\n",
    "    word_embeddings = word_model(batch)          \n",
    "    print(\"Embedded IDs Shape:\", word_embeddings.shape)\n",
    "    \n",
    "    char_embeddings = char_model(batch)\n",
    "    print(\"char_embeddings Shape:\", char_embeddings.shape)\n",
    "    \n",
    "    word_char_embeddings = torch.cat((word_embeddings, char_embeddings), 2)\n",
    "    print(\"word_char_embeddings Shape:\", word_char_embeddings.shape)\n",
    "    \n",
    "    context_embeddings = context_model(word_char_embeddings, batch['sentence_length'])\n",
    "    print(\"context_embeddings Shape:\", context_embeddings.shape)\n",
    "    \n",
    "    if 'labels' in batch:\n",
    "        labels = batch['labels']\n",
    "        print(\"Labels Shape:\", labels.shape)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-11T10:24:51.807814840Z",
     "start_time": "2024-04-11T10:24:51.719332645Z"
    }
   },
   "id": "9e2e8a3c05ccf934",
   "execution_count": 1245
  },
  {
   "cell_type": "markdown",
   "source": [
    "emissions (output of a BiLSTM or other sequence encoder) \n",
    "https://towardsdatascience.com/implementing-a-linear-chain-conditional-random-field-crf-in-pytorch-16b0b9c4b4ea\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "765b989e080929b4"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class CRF(nn.Module):\n",
    "    def __init__(self, nb_labels, cls_tag_id, sep_tag_id, pad_tag_id):\n",
    "        super(CRF, self).__init__()    \n",
    "        self.nb_labels = nb_labels\n",
    "        self.CLS_TAG_ID = cls_tag_id\n",
    "        self.SEP_TAG_ID = sep_tag_id\n",
    "        self.PAD_TAG_ID = pad_tag_id\n",
    "        self.transitions = nn.Parameter(torch.empty(self.nb_labels, self.nb_labels))\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        # initialize transitions from a random uniform distribution between -0.1 and 0.1\n",
    "        nn.init.uniform_(self.transitions, -0.1, 0.1)\n",
    "        # no transitions allowed to the beginning or from the end of sentence, from or to padding\n",
    "        self.transitions.data[:, self.CLS_TAG_ID] = -10000.0\n",
    "        self.transitions.data[self.SEP_TAG_ID, :] = -10000.0\n",
    "        self.transitions.data[self.PAD_TAG_ID, :] = -10000.0\n",
    "        self.transitions.data[:, :] = -10000.0\n",
    "        # or we are already in a pad position\n",
    "        self.transitions.data[self.PAD_TAG_ID, self.EOS_TAG_ID] = 0.0\n",
    "        self.transitions.data[self.PAD_TAG_ID, self.PAD_TAG_ID] = 0.0\n",
    "        \n",
    "    def forward(self, emissions, labels, mask=None):\n",
    "        \"\"\"Compute the negative log-likelihood. See `log_likelihood` method.\"\"\"\n",
    "        neg_likelihood = -self.log_likelihood(emissions, labels, mask=mask)\n",
    "        return neg_likelihood\n",
    "\n",
    "    def log_likelihood(self, emissions, labels, mask=None):\n",
    "        \"\"\"Compute the probability of a sequence of tags given a sequence of\n",
    "        emissions scores.\n",
    "        Args:\n",
    "            emissions (torch.Tensor): Sequence of emissions for each label.\n",
    "                Shape of (batch_size, seq_len, nb_labels)\n",
    "            labels (torch.LongTensor): Sequence of labels.\n",
    "                Shape of (batch_size, seq_len).\n",
    "            mask (torch.FloatTensor, optional): Tensor representing valid positions.\n",
    "                If None, all positions are considered valid.\n",
    "                Shape of (batch_size, seq_len).\n",
    "        Returns:\n",
    "            torch.Tensor: the (summed) log-likelihoods of each sequence in the batch.\n",
    "                Shape of (1,)\n",
    "        \"\"\"  \n",
    "        if mask is None:\n",
    "            mask = torch.ones(emissions.shape[:2], dtype=torch.float)\n",
    "    \n",
    "        scores = self._compute_scores(emissions, labels, mask=mask)\n",
    "        partition = self._compute_log_partition(emissions, mask=mask)\n",
    "        return torch.sum(scores - partition)\n",
    "\n",
    "    def _compute_scores(self, emissions, labels, mask):\n",
    "        \"\"\"Compute the scores for a given batch of emissions with their tags.\n",
    "        Args:\n",
    "            emissions (torch.Tensor): (batch_size, seq_len, nb_labels)\n",
    "            labels (Torch.LongTensor): (batch_size, seq_len)\n",
    "            mask (Torch.FloatTensor): (batch_size, seq_len)\n",
    "        Returns:\n",
    "            torch.Tensor: Scores for each batch.\n",
    "                Shape of (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length = labels.shape\n",
    "        scores = torch.zeros(batch_size)\n",
    "    \n",
    "        # save first and last tags to be used later\n",
    "        first_tags = labels[:, 0]\n",
    "        last_valid_idx = mask.int().sum(1) - 1\n",
    "        last_tags = labels.gather(1, last_valid_idx.unsqueeze(1)).squeeze()\n",
    "    \n",
    "        # add the transition from BOS to the first tags for each batch\n",
    "        t_scores = self.transitions[self.BOS_TAG_ID, first_tags]\n",
    "    \n",
    "        # add the [unary] emission scores for the first tags for each batch\n",
    "        # for all batches, the first word, see the correspondent emissions\n",
    "        # for the first tags (which is a list of ids):\n",
    "        # emissions[:, 0, [tag_1, tag_2, ..., tag_nblabels]]\n",
    "        e_scores = emissions[:, 0].gather(1, first_tags.unsqueeze(1)).squeeze()\n",
    "    \n",
    "        # the scores for a word is just the sum of both scores\n",
    "        scores += e_scores + t_scores\n",
    "    \n",
    "        # now lets do this for each remaining word\n",
    "        for i in range(1, seq_length):\n",
    "    \n",
    "            # we could: iterate over batches, check if we reached a mask symbol\n",
    "            # and stop the iteration, but vecotrizing is faster due to gpu,\n",
    "            # so instead we perform an element-wise multiplication\n",
    "            is_valid = mask[:, i]\n",
    "    \n",
    "            previous_tags = labels[:, i - 1]\n",
    "            current_tags = labels[:, i]\n",
    "    \n",
    "            # calculate emission and transition scores as we did before\n",
    "            e_scores = emissions[:, i].gather(1, current_tags.unsqueeze(1)).squeeze()\n",
    "            t_scores = self.transitions[previous_tags, current_tags]\n",
    "    \n",
    "            # apply the mask\n",
    "            e_scores = e_scores * is_valid\n",
    "            t_scores = t_scores * is_valid\n",
    "    \n",
    "            scores += e_scores + t_scores\n",
    "    \n",
    "        # add the transition from the end tag to the EOS tag for each batch\n",
    "        scores += self.transitions[last_tags, self.EOS_TAG_ID]\n",
    "    \n",
    "        return scores\n",
    "\n",
    "    def _compute_log_partition(self, emissions, mask):\n",
    "        \"\"\"Compute the partition function in log-space using the forward-algorithm.\n",
    "        Args:\n",
    "            emissions (torch.Tensor): (batch_size, seq_len, nb_labels)\n",
    "            mask (Torch.FloatTensor): (batch_size, seq_len)\n",
    "        Returns:\n",
    "            torch.Tensor: the partition scores for each batch.\n",
    "                Shape of (batch_size,)\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, nb_labels = emissions.shape\n",
    "    \n",
    "        # in the first iteration, BOS will have all the scores\n",
    "        alphas = self.transitions[self.BOS_TAG_ID, :].unsqueeze(0) + emissions[:, 0]\n",
    "    \n",
    "        for i in range(1, seq_length):\n",
    "            alpha_t = []\n",
    "    \n",
    "            for tag in range(nb_labels):\n",
    "    \n",
    "                # get the emission for the current tag\n",
    "                e_scores = emissions[:, i, tag]\n",
    "    \n",
    "                # broadcast emission to all labels\n",
    "                # since it will be the same for all previous tags\n",
    "                # (bs, nb_labels)\n",
    "                e_scores = e_scores.unsqueeze(1)\n",
    "    \n",
    "                # transitions from something to our tag\n",
    "                t_scores = self.transitions[:, tag]\n",
    "    \n",
    "                # broadcast the transition scores to all batches\n",
    "                # (bs, nb_labels)\n",
    "                t_scores = t_scores.unsqueeze(0)\n",
    "    \n",
    "                # combine current scores with previous alphas\n",
    "                # since alphas are in log space (see logsumexp below),\n",
    "                # we add them instead of multiplying\n",
    "                scores = e_scores + t_scores + alphas\n",
    "    \n",
    "                # add the new alphas for the current tag\n",
    "                alpha_t.append(torch.logsumexp(scores, dim=1))\n",
    "    \n",
    "            # create a torch matrix from alpha_t\n",
    "            # (bs, nb_labels)\n",
    "            new_alphas = torch.stack(alpha_t).t()\n",
    "    \n",
    "            # set alphas if the mask is valid, otherwise keep the current values\n",
    "            is_valid = mask[:, i].unsqueeze(-1)\n",
    "            alphas = is_valid * new_alphas + (1 - is_valid) * alphas\n",
    "    \n",
    "        # add the scores for the final transition\n",
    "        last_transition = self.transitions[:, self.EOS_TAG_ID]\n",
    "        end_scores = alphas + last_transition.unsqueeze(0)\n",
    "    \n",
    "        # return a *log* of sums of exps\n",
    "        return torch.logsumexp(end_scores, dim=1)\n",
    "\n",
    "    def _viterbi_decode(self, emissions, mask):\n",
    "        \"\"\"Compute the viterbi algorithm to find the most probable sequence of labels\n",
    "        given a sequence of emissions.\n",
    "\n",
    "        Args:\n",
    "            emissions (torch.Tensor): (batch_size, seq_len, nb_labels)\n",
    "            mask (Torch.FloatTensor): (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: the viterbi score for the for each batch.\n",
    "                Shape of (batch_size,)\n",
    "            list of lists of ints: the best viterbi sequence of labels for each batch\n",
    "        \"\"\"\n",
    "        batch_size, seq_length, nb_labels = emissions.shape\n",
    "\n",
    "        # in the first iteration, BOS will have all the scores and then, the max\n",
    "        alphas = self.transitions[self.BOS_TAG_ID, :].unsqueeze(0) + emissions[:, 0]\n",
    "\n",
    "        backpointers = []\n",
    "\n",
    "        for i in range(1, seq_length):\n",
    "            alpha_t = []\n",
    "            backpointers_t = []\n",
    "\n",
    "            for tag in range(nb_labels):\n",
    "\n",
    "                # get the emission for the current tag and broadcast to all labels\n",
    "                e_scores = emissions[:, i, tag]\n",
    "                e_scores = e_scores.unsqueeze(1)\n",
    "\n",
    "                # transitions from something to our tag and broadcast to all batches\n",
    "                t_scores = self.transitions[:, tag]\n",
    "                t_scores = t_scores.unsqueeze(0)\n",
    "\n",
    "                # combine current scores with previous alphas\n",
    "                scores = e_scores + t_scores + alphas\n",
    "\n",
    "                # so far is exactly like the forward algorithm,\n",
    "                # but now, instead of calculating the logsumexp,\n",
    "                # we will find the highest score and the tag associated with it\n",
    "                max_score, max_score_tag = torch.max(scores, dim=-1)\n",
    "\n",
    "                # add the max score for the current tag\n",
    "                alpha_t.append(max_score)\n",
    "\n",
    "                # add the max_score_tag for our list of backpointers\n",
    "                backpointers_t.append(max_score_tag)\n",
    "\n",
    "            # create a torch matrix from alpha_t\n",
    "            # (bs, nb_labels)\n",
    "            new_alphas = torch.stack(alpha_t).t()\n",
    "\n",
    "            # set alphas if the mask is valid, otherwise keep the current values\n",
    "            is_valid = mask[:, i].unsqueeze(-1)\n",
    "            alphas = is_valid * new_alphas + (1 - is_valid) * alphas\n",
    "\n",
    "            # append the new backpointers\n",
    "            backpointers.append(backpointers_t)\n",
    "\n",
    "        # add the scores for the final transition\n",
    "        last_transition = self.transitions[:, self.EOS_TAG_ID]\n",
    "        end_scores = alphas + last_transition.unsqueeze(0)\n",
    "\n",
    "        # get the final most probable score and the final most probable tag\n",
    "        max_final_scores, max_final_tags = torch.max(end_scores, dim=1)\n",
    "\n",
    "        # find the best sequence of labels for each sample in the batch\n",
    "        best_sequences = []\n",
    "        emission_lengths = mask.int().sum(dim=1)\n",
    "        for i in range(batch_size):\n",
    "\n",
    "            # recover the original sentence length for the i-th sample in the batch\n",
    "            sample_length = emission_lengths[i].item()\n",
    "\n",
    "            # recover the max tag for the last timestep\n",
    "            sample_final_tag = max_final_tags[i].item()\n",
    "\n",
    "            # limit the backpointers until the last but one\n",
    "            # since the last corresponds to the sample_final_tag\n",
    "            sample_backpointers = backpointers[: sample_length - 1]\n",
    "\n",
    "            # follow the backpointers to build the sequence of labels\n",
    "            sample_path = self._find_best_path(i, sample_final_tag, sample_backpointers)\n",
    "\n",
    "            # add this path to the list of best sequences\n",
    "            best_sequences.append(sample_path)\n",
    "\n",
    "        return max_final_scores, best_sequences\n",
    "\n",
    "    def _find_best_path(self, sample_id, best_tag, backpointers):\n",
    "        \"\"\"Auxiliary function to find the best path sequence for a specific sample.\n",
    "\n",
    "            Args:\n",
    "                sample_id (int): sample index in the range [0, batch_size)\n",
    "                best_tag (int): tag which maximizes the final score\n",
    "                backpointers (list of lists of tensors): list of pointers with\n",
    "                shape (seq_len_i-1, nb_labels, batch_size) where seq_len_i\n",
    "                represents the length of the ith sample in the batch\n",
    "\n",
    "            Returns:\n",
    "                list of ints: a list of tag indexes representing the bast path\n",
    "        \"\"\"\n",
    "\n",
    "        # add the final best_tag to our best path\n",
    "        best_path = [best_tag]\n",
    "\n",
    "        # traverse the backpointers in backwards\n",
    "        for backpointers_t in reversed(backpointers):\n",
    "\n",
    "            # recover the best_tag at this timestep\n",
    "            best_tag = backpointers_t[best_tag][sample_id].item()\n",
    "\n",
    "            # append to the beginning of the list so we don't need to reverse it later\n",
    "            best_path.insert(0, best_tag)\n",
    "\n",
    "        return best_path"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "14a54b4c73b7ba73"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class BiLISTM_CRF(nn.Module):\n",
    "    def __init__(self, \n",
    "                 tag_to_ix, hidden_dim,\n",
    "                 vocab_size, word_embedding_dim, pre_word_embeds=None,                  \n",
    "                 char_to_ix=None, char_out_dimension=25, char_embedding_dim=25, char_num_layers=1,\n",
    "                 use_gpu=False, use_char=True, use_crf=True,\n",
    "                 dropout_rate=0.5):\n",
    "        \"\"\"\n",
    "        Input parameters:\n",
    "                \n",
    "                vocab_size= Size of vocabulary (int)\n",
    "                tag_to_ix = Dictionary that maps NER tags to indices\n",
    "                word_embedding_dim = Dimension of word embeddings (int)\n",
    "                hidden_dim = The hidden dimension of the LSTM layer (int)\n",
    "                char_to_ix = Dictionary that maps characters to indices\n",
    "                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n",
    "                char_out_dimension = Output dimension from the encoder for character\n",
    "                char_embedding_dim = Dimension of the character embeddings\n",
    "                use_gpu = defines availability of GPU, \n",
    "                    when True: CUDA function calls are made\n",
    "                    else: Normal CPU function calls are made\n",
    "                use_crf = parameter which decides if you want to use the CRF layer for output decoding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        self.use_gpu = use_gpu\n",
    "        #self.word_embedding_dim = word_embedding_dim\n",
    "        #self.hidden_dim = hidden_dim\n",
    "        #self.vocab_size = vocab_size\n",
    "        self.tag_to_ix = tag_to_ix\n",
    "        self.use_crf = use_crf\n",
    "        self.use_char = use_char\n",
    "        self.char_out_dimension = char_out_dimension\n",
    "        \n",
    "        if self.use_char:\n",
    "            self.char_embeds = nn.Embedding(len(self.char_to_ix), char_embedding_dim)            \n",
    "            self.char_lstm = nn.LSTM(char_embedding_dim, char_out_dimension, num_layers=char_num_layers, bidirectional=True)\n",
    "    \n",
    "        self.word_embeds = nn.Embedding(vocab_size, word_embedding_dim)\n",
    "        \n",
    "         ## ? get existent ??\n",
    "        if pre_word_embeds is not None:\n",
    "            #Initializes the word embeddings with pretrained word embeddings\n",
    "            self.pre_word_embeds = True\n",
    "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
    "        else:\n",
    "            self.pre_word_embeds = False\n",
    "        \n",
    "        #Drop out layer \n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        #BiLSTM for concatenated embeddings layer \n",
    "        self.lstm = nn.LSTM(word_embedding_dim + (char_out_dimension * 2 if use_char else 0), \n",
    "                            hidden_dim, bidirectional=True)\n",
    "        \n",
    "        # Linear layer to maps the output of the biLSTM into tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, len(tag_to_ix))\n",
    "        \n",
    "        # Initialize the matrix of transition parameters between entities\n",
    "        if self.use_crf:            \n",
    "            self.transitions = nn.Parameter(torch.zeros(len(tag_to_ix), len(tag_to_ix)))            \n",
    "            # Never transfer to the start tag or from the end tag\n",
    "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
    "            self.transitions.data[:, tag_to_ix[END_TAG]] = -10000\n",
    "\n",
    "    \n",
    "    def _get_lstm_features(self, sentence, characters, characters_lengths, d):\n",
    "        \n",
    "        #get characters embeddings\n",
    "        chars_embeds = self.char_embeds(characters).transpose(0, 1)\n",
    "        #prepaire for LSTM to avoid padding\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(chars_embeds, characters_lengths)\n",
    "        \n",
    "        lstm_out, _ = self.char_lstm(packed)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(lstm_out)\n",
    "        outputs = outputs.transpose(0, 1)\n",
    "            \n",
    "        chars_embeds_temp = torch.zeros((outputs.size(0), outputs.size(2)), dtype=torch.float32)\n",
    "            \n",
    "        if self.use_gpu:\n",
    "            chars_embeds_temp = chars_embeds_temp.cuda()\n",
    "        # cut paddings             \n",
    "        for i, index in enumerate(output_lengths):\n",
    "            chars_embeds_temp[i] = torch.cat((outputs[i, index-1, :self.char_lstm_dim], outputs[i, 0, self.char_lstm_dim:]))\n",
    "        \n",
    "        chars_embeds = chars_embeds_temp.clone()\n",
    "        \n",
    "        for i in range(chars_embeds.size(0)):\n",
    "            chars_embeds[d[i]] = chars_embeds_temp[i]\n",
    "\n",
    "    \n",
    "        ## Loading word embeddings\n",
    "        embeds = self.word_embeds(sentence)\n",
    "    \n",
    "        ## We concatenate the word embeddings and the character level representation\n",
    "        ## to create unified representation for each word\n",
    "        embeds = torch.cat((embeds, chars_embeds), 1)\n",
    "    \n",
    "        embeds = embeds.unsqueeze(1)\n",
    "    \n",
    "        ## Dropout on the unified embeddings\n",
    "        embeds = self.dropout(embeds)\n",
    "    \n",
    "        ## Word lstm\n",
    "        ## Takes words as input and generates a output at each step\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "    \n",
    "        ## Reshaping the outputs from the lstm layer\n",
    "        lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
    "    \n",
    "        ## Dropout on the lstm output\n",
    "        lstm_out = self.dropout(lstm_out)\n",
    "    \n",
    "        ## Linear layer converts the ouput vectors to tag space\n",
    "        lstm_feats = self.hidden2tag(lstm_out)\n",
    "        \n",
    "        return lstm_feats\n",
    "    \n",
    "    def viterbi_algo(self, feats):\n",
    "        '''\n",
    "        In this function, we implement the viterbi algorithm explained above.\n",
    "        A Dynamic programming based approach to find the best tag sequence\n",
    "        '''\n",
    "        backpointers = []\n",
    "        # analogous to forward\n",
    "        \n",
    "        # Initialize the viterbi variables in log space\n",
    "        init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
    "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
    "        \n",
    "        # forward_var at step i holds the viterbi variables for step i-1\n",
    "        forward_var = init_vvars\n",
    "        if self.use_gpu:\n",
    "            forward_var = forward_var.cuda()\n",
    "        for feat in feats:\n",
    "            next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
    "            _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
    "            bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n",
    "            next_tag_var = next_tag_var.data.cpu().numpy() \n",
    "            viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n",
    "            viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
    "            if self.use_gpu:\n",
    "                viterbivars_t = viterbivars_t.cuda()\n",
    "                \n",
    "            # Now add in the emission scores, and assign forward_var to the set\n",
    "            # of viterbi variables we just computed\n",
    "            forward_var = viterbivars_t + feat\n",
    "            backpointers.append(bptrs_t)\n",
    "    \n",
    "        # Transition to STOP_TAG\n",
    "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
    "        terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
    "        terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
    "        best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
    "        path_score = terminal_var[best_tag_id]\n",
    "        \n",
    "        # Follow the back pointers to decode the best path.\n",
    "        best_path = [best_tag_id]\n",
    "        for bptrs_t in reversed(backpointers):\n",
    "            best_tag_id = bptrs_t[best_tag_id]\n",
    "            best_path.append(best_tag_id)\n",
    "            \n",
    "        # Pop off the start tag (we dont want to return that to the caller)\n",
    "        start = best_path.pop()\n",
    "        assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
    "        best_path.reverse()\n",
    "        return path_score, best_path\n",
    "    \n",
    "    def forward(self, sentence, chars, chars2_length, d):\n",
    "        feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
    "        \n",
    "        # Find the best path, given the features.\n",
    "        if self.use_crf:\n",
    "            score, tag_seq = self.viterbi_decode(feats)\n",
    "        else:\n",
    "            score, tag_seq = torch.max(feats, 1)\n",
    "            tag_seq = list(tag_seq.cpu().data)\n",
    "    \n",
    "        return score, tag_seq"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-04-05T08:24:15.887893755Z",
     "start_time": "2024-04-05T08:24:15.847075569Z"
    }
   },
   "id": "e674c02dbb1bd230",
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
